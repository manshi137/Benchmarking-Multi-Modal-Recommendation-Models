{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ml-100k \\ text \\ items.csv -----> baby \\ text_feat.npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'items.csv'\n",
    "\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data_reader = csv.reader(file)\n",
    "    data = [row for row in data_reader]\n",
    "\n",
    "# for i in range(len(data[0])):\n",
    "#     print(\"i: \", i)\n",
    "#     print(\"data[0][i]: \", data[0][i])\n",
    "ind1 = [5, 29, 30, 31, 32]\n",
    "# movie title-5, summary-28, cast-29, director-30, rating-31, runtime-32\n",
    "\n",
    "moviedesc = []\n",
    "for i in range(1, len(data)):\n",
    "    tmp=\"\"\n",
    "    for j in ind1:\n",
    "        tmp += data[i][j]\n",
    "        tmp += \" \"\n",
    "    # genres\n",
    "    for j in range(10, 28):\n",
    "        if(data[i][j]=='1'):\n",
    "            tmp += data[0][j]\n",
    "            tmp += \" \"\n",
    "    moviedesc.append(tmp)\n",
    "\n",
    "print(\"len(moviedesc): \", len(moviedesc))\n",
    "print(moviedesc)\n",
    "\n",
    "# making embeddings using huggingface library\n",
    "\n",
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = tokenizer(moviedesc, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "text_embeds = outputs.text_embeds\n",
    "# print(text_embeds.size())\n",
    "print(\"len(text_embeds): \", len(text_embeds))\n",
    "resized_text_embeds = []\n",
    "\n",
    "\n",
    "# resizing text embeddings to 384 \n",
    "for t in text_embeds:\n",
    "    resized_t = torch.nn.functional.interpolate(t.unsqueeze(0).unsqueeze(0), size = (384,), mode = 'linear').squeeze()\n",
    "    resized_text_embeds.append(resized_t)\n",
    "\n",
    "print(\"len(resized_text_embeds): \", len(resized_text_embeds))\n",
    "# saving text embeddings in text_feat.npy\n",
    "numpy_array = [tensor.detach().numpy() for tensor in resized_text_embeds]\n",
    "print(\"len(numpy_array): \", len(numpy_array))\n",
    "stacked_resized_text_embeds = np.stack(numpy_array, axis=0)\n",
    "print(\"stacked_resized_text_embeds.shape: \", stacked_resized_text_embeds.shape)\n",
    "np.save('text_feat.npy',stacked_resized_text_embeds)\n",
    "print(\"saved text embeddings in text_feat.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting middle frame from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_middle_frame(video_path, output_folder):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video '{video_path}'\")\n",
    "        return\n",
    "\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the middle frame index\n",
    "    middle_frame_index = total_frames // 2\n",
    "\n",
    "    # Set the video capture object to the middle frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_index)\n",
    "\n",
    "    # Read the middle frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame was read successfully\n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read middle frame from video '{video_path}'\")\n",
    "        return\n",
    "\n",
    "    # Close the video file\n",
    "    cap.release()\n",
    "\n",
    "    # Save the middle frame as an image\n",
    "    frame_filename = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(video_path))[0]}_middle_frame.jpg\")\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    print(f\"Saved the middle frame of '{video_path}' as '{frame_filename}'\")\n",
    "\n",
    "# Specify the path to your video dataset folder\n",
    "video_dataset_folder = \"C:/Users/richi/OneDrive/Documents/COL865_MMRS/videos\"\n",
    "\n",
    "# Specify the folder where you want to save the extracted frames\n",
    "output_image_folder = \"C:/Users/richi/OneDrive/Documents/COL865_MMRS/images\"\n",
    "\n",
    "# Loop through the video files in the dataset folder\n",
    "for video_filename in os.listdir(video_dataset_folder):\n",
    "    video_path = os.path.join(video_dataset_folder, video_filename)\n",
    "    extract_middle_frame(video_path, output_image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converting extracted images into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "from PIL import Image\n",
    "# import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "jpg_dir = \"C:/Users/richi/OneDrive/Documents/COL865_MMRS/images\"\n",
    "\n",
    "\n",
    "model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "for filename in os.listdir(jpg_dir):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "        file_path = os.path.join(jpg_dir, filename)\n",
    "        image_sample =Image.open(file_path)\n",
    "        inputs = processor(images=image_sample, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        image_embeds = outputs.image_embeds\n",
    "        print(image_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "# import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "# jpg_dir =  \"/Users/manshisagar/Desktop/sem-7 assign/col865/project/COL865_MMRS/ml-100k/Image\"\n",
    "# ekansh\n",
    "jpg_dir = \"/Users/manshisagar/Desktop/sem-7assign/col865/project/COL865_MMRS/movie_posters_resized\"\n",
    "\n",
    "model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# def resize_and_preprocess_image(image_path, target_size=(256, 256)):\n",
    "#     image = Image.open(image_path)\n",
    "#     image = image.resize(target_size)\n",
    "#     inputs = processor(images=image, return_tensors=\"pt\")\n",
    "#     return inputs\n",
    "\n",
    "image_files = [filename for filename in os.listdir(jpg_dir) if filename.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "print(len(image_files))\n",
    "\n",
    "\n",
    "\n",
    "arr_image_embeds= []\n",
    "\n",
    "# Loop through all the image files and process them\n",
    "for i in range(len(image_files)):\n",
    "    # print(filename)\n",
    "    filename = image_files[i]\n",
    "    print(i)\n",
    "    file_path = os.path.join(jpg_dir, filename)\n",
    "    image_sample = Image.open(file_path)\n",
    "    inputs = processor(images=image_sample, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    image_embeds = outputs.image_embeds\n",
    "    arr_image_embeds.append(image_embeds)\n",
    "\n",
    "# for i in range(len(image_files)):\n",
    "#     filename = image_files[i]\n",
    "#     print(i)\n",
    "#     file_path = os.path.join(jpg_dir, filename)\n",
    "#     inputs = resize_and_preprocess_image(file_path)\n",
    "#     outputs = model(**inputs)\n",
    "#     image_embeds = outputs.image_embeds\n",
    "#     arr_image_embeds.append(image_embeds)\n",
    "\n",
    "print(\"size of tensor == \", len(arr_image_embeds[0]))\n",
    "print(\"size of arr_image_embeds== \", len(arr_image_embeds))\n",
    "new_image_embeds= []\n",
    "for t in arr_image_embeds:\n",
    "    temp= t.squeeze()\n",
    "    new_image_embeds.append(temp)\n",
    "\n",
    "print(\"size of tensor == \", len(new_image_embeds[0]))\n",
    "print(\"size of new_image_embeds== \", len(new_image_embeds))\n",
    "resized_image_embeds= []\n",
    "# resizing text embeddings to 384 \n",
    "for t in new_image_embeds:\n",
    "    resized_i = torch.nn.functional.interpolate(t.unsqueeze(0).unsqueeze(0), size = (4096,), mode = 'linear').squeeze()\n",
    "    resized_image_embeds.append(resized_i)\n",
    "\n",
    "print(\"size of new tensor == \", resized_image_embeds[0].size())\n",
    "print(\"size of resized_image_embeds== \", len(resized_image_embeds))\n",
    "# saving text embeddings in text_feat.npy\n",
    "numpy_array = [tensor.detach().numpy() for tensor in resized_image_embeds]\n",
    "\n",
    "stacked_resized_text_embeds = np.stack(numpy_array, axis=0)\n",
    "\n",
    "np.save('images_feat.npy',stacked_resized_text_embeds)\n",
    "print(\"saved text embeddings in image_feat.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
