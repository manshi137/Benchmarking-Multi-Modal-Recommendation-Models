{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "\n",
    "# Set the path to your image folder\n",
    "image_folder_path = \"movie_posters\"  # Replace with the path to your \"movie_posters\" folder\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the transformation to preprocess the images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "embeddings = []\n",
    "name2embedding = {}\n",
    "id2name = {}\n",
    "\n",
    "# Get a list of all image file paths in the folder\n",
    "image_paths = [os.path.join(image_folder_path, file) for file in os.listdir(image_folder_path) if file.endswith(\".jpg\")]\n",
    "\n",
    "print(image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process images one by one and generate embeddings\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path)\n",
    "    # print(\"Processing:\", image_path)\n",
    "    try:\n",
    "        image = preprocess(image)\n",
    "        image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = resnet(image)\n",
    "        embeddings.append(embedding)\n",
    "        name2embedding[image_path[14:-4]] = embedding\n",
    "    except:\n",
    "        print(\"Error processing:\", image_path)\n",
    "    print(len(embeddings))\n",
    "        \n",
    "\n",
    "print(name2embedding)\n",
    "\n",
    "# 'embeddings' now contains the feature vectors for all images in the \"movie_posters\" folder\n",
    "embeddings = torch.cat(embeddings, 0)\n",
    "\n",
    "# Now, 'embeddings' contains the feature vectors for all images in the \"movie_posters\" folder\n",
    "print(embeddings.shape)  # The shape will be (num_images, 1000) for ResNet-50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "file_path = 'items.csv'\n",
    "file = open(file_path, encoding=\"utf8\")\n",
    "\n",
    "data_reader = csv.reader(file)\n",
    "data = [row for row in data_reader]\n",
    "\n",
    "id2name = {}\n",
    "moviedesc = []\n",
    "movie_names = []\n",
    "\n",
    "\n",
    "for i in range(1, len(data)):\n",
    "    movie_names.append(data[i][5])\n",
    "    # print({i-1}, {data[i][5]})\n",
    "\n",
    "for i in range(len(movie_names)):\n",
    "    movie_names[i] = movie_names[i].strip()\n",
    "    movie_names[i] = movie_names[i][:-6]\n",
    "    movie_names[i] = movie_names[i].strip()\n",
    "\n",
    "for i in range(len(movie_names)):\n",
    "    tmp = tmp = re.sub(r'[\\\\/:\"*?<>|]', '', movie_names[i])\n",
    "    id2name[i] = str(i)+\"_\"+tmp\n",
    "\n",
    "print(id2name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = []\n",
    "for i in range(len(movie_names)):\n",
    "    image_embeddings.append(name2embedding[id2name[i]])\n",
    "    print(\"Processing:\", id2name[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "new_image_embeds = []\n",
    "print(len(image_embeddings))\n",
    "print(image_embeddings[0].shape)\n",
    "for t in image_embeddings:\n",
    "    temp= t.squeeze()\n",
    "    new_image_embeds.append(temp)\n",
    "\n",
    "print(\"size of tensor == \", len(new_image_embeds[0]))\n",
    "print(\"size of new_image_embeds== \", len(new_image_embeds))\n",
    "resized_image_embeds= []\n",
    "# resizing text embeddings to 384 \n",
    "for t in new_image_embeds:\n",
    "    resized_i = torch.nn.functional.interpolate(t.unsqueeze(0).unsqueeze(0), size = (4096,), mode = 'linear').squeeze()\n",
    "    resized_image_embeds.append(resized_i)\n",
    "\n",
    "print(\"size of new tensor == \", resized_image_embeds[0].size())\n",
    "print(\"size of resized_image_embeds== \", len(resized_image_embeds))\n",
    "# saving text embeddings in text_feat.npy\n",
    "numpy_array = [tensor.detach().numpy() for tensor in resized_image_embeds]\n",
    "\n",
    "stacked_resized_text_embeds = np.stack(numpy_array, axis=0)\n",
    "\n",
    "np.save('data/ml100k/image_feat.npy',stacked_resized_text_embeds)\n",
    "print(\"saved text embeddings in image_feat.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
