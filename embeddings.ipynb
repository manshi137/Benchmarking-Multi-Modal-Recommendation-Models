{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import librosa\n",
    "from transformers import AutoProcessor, XCLIPVisionModel, CLIPVisionModelWithProjection, ClapAudioModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "text_input = [\"Your text data goes here.\"]\n",
    "text_encoded = clip.tokenize(text_input).to(device)\n",
    "text_features = model.encode_text(text_encoded)\n",
    "\n",
    "# The text embeddings are stored in text_features\n",
    "print(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "text_embeds = outputs.text_embeds\n",
    "print(text_embeds)\n",
    "print(len(text_embeds[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting first frames from videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_frame(video_path, output_folder):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video '{video_path}'\")\n",
    "        return\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame was read successfully\n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read frame from video '{video_path}'\")\n",
    "        return\n",
    "\n",
    "    # Close the video file\n",
    "    cap.release()\n",
    "\n",
    "    # Save the frame as an image\n",
    "    frame_filename = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(video_path))[0]}.jpg\")\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    # print(f\"Saved the first frame of '{video_path}' as '{frame_filename}'\")\n",
    "\n",
    "\n",
    "# Specify the path to your video dataset folder\n",
    "video_dataset_folder = \"/home/ekansh/Desktop/COL865_MMRS/ml-100k/Video\"\n",
    "\n",
    "# Specify the folder where you want to save the extracted frames\n",
    "output_image_folder = \"/home/ekansh/Desktop/COL865_MMRS/ml-100k/Image\"\n",
    "\n",
    "# Loop through the video files in the dataset folder\n",
    "for video_filename in os.listdir(video_dataset_folder):\n",
    "    video_path = os.path.join(video_dataset_folder, video_filename)\n",
    "    extract_first_frame(video_path, output_image_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manshi\n",
    "# jpg_dir =  \"/Users/manshisagar/Desktop/sem-7 assign/col865/project/COL865_MMRS/ml-100k/Image\"\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "image_embeds = outputs.image_embeds\n",
    "\n",
    "print(image_embeds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "from PIL import Image\n",
    "# import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "jpg_dir =  \"/Users/manshisagar/Desktop/sem-7 assign/col865/project/COL865_MMRS/ml-100k/Image\"\n",
    "# ekansh\n",
    "jpg_dir = \"/home/ekansh/Desktop/COL865_MMRS/ml-100k/Image\"\n",
    "\n",
    "model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "for filename in os.listdir(jpg_dir):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "        file_path = os.path.join(jpg_dir, filename)\n",
    "        image_sample =Image.open(file_path)\n",
    "        inputs = processor(images=image_sample, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        image_embeds = outputs.image_embeds\n",
    "        print(image_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manshi\n",
    "# data_dir = \"/Users/manshisagar/Desktop/sem-7 assign/col865/project/COL865_MMRS/ml-100k/Audio\"\n",
    "\n",
    "# ekansh\n",
    "data_dir = \"/home/ekansh/Desktop/COL865_MMRS/ml-100k/Audio\"\n",
    "\n",
    "model = ClapAudioModel.from_pretrained(\"laion/clap-htsat-fused\")\n",
    "processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        audio_sample, sample_rate = librosa.load(file_path, sr=None)\n",
    "        inputs = processor(audios=audio_sample, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        print(last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "file_path = hf_hub_download(repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\")\n",
    "container = av.open(file_path)\n",
    "# sample 16 frames\n",
    "indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model = XCLIPVisionModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "\n",
    "pixel_values = processor(videos=list(video), return_tensors=\"pt\").pixel_values\n",
    "\n",
    "batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
    "pixel_values = pixel_values.reshape(-1, num_channels, height, width)\n",
    "\n",
    "outputs = model(pixel_values)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "print(\"last_hidden_state\")\n",
    "print(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, XCLIPVisionModel\n",
    "# from huggingface_hub import hf_hub_download\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model = XCLIPVisionModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "\n",
    "mp4_dir = \"/Users/manshisagar/Desktop/sem-7 assign/col865/project/COL865_MMRS/ml-100k/Video\"\n",
    "\n",
    "for filename in os.listdir(mp4_dir):\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        container = av.open(file_path)\n",
    "        # sample 16 frames\n",
    "        # -------------\n",
    "        \n",
    "        # 1. Desired duration for the video segment in seconds\n",
    "        desired_duration_seconds = 1  # Adjust this to your desired duration\n",
    "\n",
    "        # 2. Total number of frames in the video\n",
    "        total_frames = container.streams.video[0].frames\n",
    "\n",
    "        # 3. Calculate frame_sample_rate based on desired duration and total frames\n",
    "        frame_sample_rate = total_frames / (desired_duration_seconds * container.streams.video[0].time_base)\n",
    "\n",
    "        # 4. Calculate clip_len\n",
    "        clip_len = int(frame_sample_rate * desired_duration_seconds)\n",
    "        # -----------\n",
    "        indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate , seg_len=container.streams.video[0].frames)\n",
    "        video = read_video_pyav(container, indices)\n",
    "\n",
    "        pixel_values = processor(videos=list(video), return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
    "        pixel_values = pixel_values.reshape(-1, num_channels, height, width)\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        # print(\"outputs\")\n",
    "        # print(outputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        print(\"last_hidden_state\")\n",
    "        print(last_hidden_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.10"
=======
   "version": "3.11.5"
>>>>>>> 6351dd27d58602f7a4deeefb1ed1d1eb178a3da3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
